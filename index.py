# Copyright (C) 2025 OrPudding
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

import re
import io
import sys
import requests
import logging
from flask import Flask, request, jsonify, Response, render_template_string
from urllib.parse import urlencode, quote, unquote
from typing import List, Dict, Optional, Tuple
from bs4 import BeautifulSoup
from PIL import Image
from cachetools import cached, TTLCache
from concurrent.futures import ThreadPoolExecutor, as_completed

# ==============================================================================
# ç¼“å­˜é…ç½®
# ==============================================================================
list_cache = TTLCache(maxsize=100, ttl=300 )
gallery_cache = TTLCache(maxsize=500, ttl=3600)
image_proxy_cache = TTLCache(maxsize=1000, ttl=86400)
pagination_cache = TTLCache(maxsize=200, ttl=600)

import sys
sys.stdout.reconfigure(encoding="utf-8")

def decode_search_value(value: str) -> str:
    """
    åˆ¤æ–­å¹¶è§£ç æœç´¢å€¼
    å¦‚æœå€¼æ˜¯URLç¼–ç ï¼Œåˆ™è§£ç ä¸ºä¸­æ–‡ï¼Œå¦åˆ™ç›´æ¥è¿”å›
    """
    # URLç¼–ç çš„ç‰¹å¾ï¼šåŒ…å«%åè·Ÿä¸¤ä¸ªåå…­è¿›åˆ¶å­—ç¬¦
    url_encoded_pattern = r"%[0-9A-Fa-f]{2}"

    # å¦‚æœåŒ…å«URLç¼–ç ç‰¹å¾ï¼Œå°è¯•è§£ç 
    if re.search(url_encoded_pattern, value):
        try:
            decoded = unquote(value)
            # è§£ç åå¦‚æœè¿˜åŒ…å«URLç¼–ç ç‰¹å¾ï¼Œè¯´æ˜å¯èƒ½æœ‰å¤šé‡ç¼–ç ï¼Œç»§ç»­è§£ç 
            while re.search(url_encoded_pattern, decoded):
                temp = unquote(decoded)
                if temp == decoded:  # å¦‚æœæ²¡æœ‰å˜åŒ–ï¼Œåœæ­¢è§£ç 
                    break
                decoded = temp
            return decoded
        except Exception:
            # å¦‚æœè§£ç å¤±è´¥ï¼Œè¿”å›åŸå€¼
            return value
    else:
        # æ²¡æœ‰URLç¼–ç ç‰¹å¾ï¼Œç›´æ¥è¿”å›
        return value

# ==============================================================================
# æ¨¡å— 1: E-Hentai HTML è§£æå™¨ (EhParser)
# ==============================================================================
class EhParser:
    PATTERN_GALLERY_URL = re.compile(r'/g/(\d+)/([a-f0-9]+)/?')
    PATTERN_RATING = re.compile(r'background-position:\s*(-?\d+)px')
    PATTERN_PAGES = re.compile(r'(\d+)\s+pages?', re.IGNORECASE)
    PATTERN_NEXT_ID = re.compile(r'[?&]next=(\d+)')
    PATTERN_STYLE_DETAILS = re.compile(r'width:(\d+)px;height:(\d+)px;.*background:.*?url\(([^)]+)\) (-?\d+)px (-?\d+)')

    @staticmethod
    def parse_gallery_list(html: str) -> Dict:
        soup = BeautifulSoup(html, 'html.parser')
        galleries = []
        main_table = soup.find('table', class_='itg gltc')
        # å¦‚æœæ‰¾ä¸åˆ°ä¸»è¡¨æ ¼ï¼Œè®°å½•æ—¥å¿—å¹¶è¿”å›ç©ºç»“æœ
        if not main_table:
            logging.warning("æœªèƒ½è§£æåˆ°ç”»å»Šåˆ—è¡¨ (æ‰¾ä¸åˆ° 'itg gltc' è¡¨æ ¼)ã€‚é¡µé¢åŸå§‹å†…å®¹å¦‚ä¸‹ï¼š")
            logging.debug(html)
            return {'galleries': [], 'pagination': {}}
        
        rows = main_table.find_all('tr')
        for row in rows:
            try:
                name_cell = row.find('td', class_='glname')
                if not name_cell: continue
                gallery = {}
                link_tag = name_cell.find('a')
                if not link_tag or 'href' not in link_tag.attrs: continue
                url = link_tag['href']
                match = EhParser.PATTERN_GALLERY_URL.search(url)
                if not match: continue
                gallery['gid'] = int(match.group(1)); gallery['token'] = match.group(2); gallery['url'] = url
                title_div = link_tag.find('div', class_='glink')
                gallery['title'] = title_div.text.strip() if title_div else 'N/A'
                thumb_cell = row.find('td', class_='gl2c')
                if thumb_cell:
                    img_tag = thumb_cell.find('img')
                    if img_tag: gallery['thumbnail'] = img_tag.get('data-src') or img_tag.get('src')
                    posted_div = thumb_cell.find('div', id=lambda x: x and x.startswith('posted_'))
                    if posted_div: gallery['posted'] = posted_div.text.strip()
                category_cell = row.find('td', class_='glcat')
                if category_cell: gallery['category'] = category_cell.text.strip()
                rating_elem = name_cell.find('div', class_='ir')
                if rating_elem:
                    style = rating_elem.get('style', ''); rating_match = EhParser.PATTERN_RATING.search(style)
                    if rating_match: gallery['rating'] = round(5 - (abs(int(rating_match.group(1))) / 16.0), 2)
                uploader_cell = row.find('td', class_='glhide')
                if uploader_cell:
                    uploader_elem = uploader_cell.find('a')
                    if uploader_elem: gallery['uploader'] = uploader_elem.get_text(strip=True)
                    pages_text_node = uploader_cell.find(string=re.compile(r'\d+\s+pages?'))
                    if pages_text_node:
                        pages_match = EhParser.PATTERN_PAGES.search(pages_text_node)
                        if pages_match: gallery['pages'] = int(pages_match.group(1))
                galleries.append(gallery)
            except Exception as e: logging.error(f"è§£æç”»å»Šé¡¹æ—¶å‘ç”Ÿé”™è¯¯: {e}"); continue
        
        # å¦‚æœå¾ªç¯ååˆ—è¡¨ä»ä¸ºç©ºï¼Œå¯èƒ½é¡µé¢æœ‰å†…å®¹ä½†æ‰€æœ‰è¡Œéƒ½è§£æå¤±è´¥
        if not galleries and len(rows) > 1: # len(rows) > 1 æ˜¯ä¸ºäº†æ’é™¤åªæœ‰è¡¨å¤´çš„æƒ…å†µ
            logging.warning("ç”»å»Šåˆ—è¡¨è§£æç»“æœä¸ºç©ºï¼Œå¯èƒ½æ‰€æœ‰è¡Œéƒ½è§£æå¤±è´¥ã€‚é¡µé¢åŸå§‹å†…å®¹å¦‚ä¸‹ï¼š")
            logging.debug(html)

        pagination = {'has_next': False, 'next_id': None}
        try:
            pager = soup.find('div', class_='searchnav') or soup.find('table', class_='ptt')
            if pager:
                next_link = pager.find('a', id='unext') or pager.find('a', text='>')
                if next_link and next_link.has_attr('href'):
                    pagination['has_next'] = True
                    href = next_link['href']
                    next_id_match = EhParser.PATTERN_NEXT_ID.search(href)
                    if next_id_match: pagination['next_id'] = next_id_match.group(1)
        except Exception as e: logging.error(f"è§£æåˆ†é¡µä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return {'galleries': galleries, 'pagination': pagination}

    @staticmethod
    def parse_gallery_detail(html: str) -> Dict:
        soup = BeautifulSoup(html, 'html.parser'); detail = {}
        try:
            # æ£€æŸ¥æ ¸å¿ƒå…ƒç´ æ˜¯å¦å­˜åœ¨
            if not soup.select_one('#gn') and not soup.select_one('#gj'):
                logging.warning("æœªèƒ½è§£æåˆ°ç”»å»Šè¯¦æƒ… (æ‰¾ä¸åˆ°æ ‡é¢˜å…ƒç´  #gn æˆ– #gj)ã€‚é¡µé¢åŸå§‹å†…å®¹å¦‚ä¸‹ï¼š")
                logging.debug(html)
                return {}

            title_elem = soup.select_one('#gn');
            if title_elem: detail['title'] = title_elem.get_text(strip=True)
            title_jp_elem = soup.select_one('#gj');
            if title_jp_elem: detail['title_jp'] = title_jp_elem.get_text(strip=True)
            category_elem = soup.select_one('#gdc a');
            if category_elem: detail['category'] = category_elem.get_text(strip=True)
            thumb_elem = soup.select_one('#gd1 div')
            if thumb_elem:
                style = thumb_elem.get('style', ''); url_match = re.search(r'url\((.+?)\)', style)
                if url_match: detail['thumbnail'] = url_match.group(1)
            tags = {}
            tag_list = soup.select('#taglist tr')
            for tag_row in tag_list:
                tag_type = tag_row.select_one('td.tc')
                if tag_type:
                    tag_name = tag_type.get_text(strip=True).rstrip(':'); tag_values = [tag_elem.get_text(strip=True) for tag_elem in tag_row.select('td div a')]
                    if tag_values: tags[tag_name] = tag_values
            if tags: detail['tags'] = tags
            rating_elem = soup.select_one('#rating_label')
            if rating_elem:
                rating_text = rating_elem.get_text(strip=True); rating_match = re.search(r'([\d.]+)', rating_text)
                if rating_match: detail['rating'] = float(rating_match.group(1))
            
            gdd_rows = soup.select('#gdd tr')
            for row in gdd_rows:
                label_elem = row.select_one('td.gdt1')
                value_elem = row.select_one('td.gdt2')
                if label_elem and value_elem:
                    label_text = label_elem.get_text(strip=True)
                    if 'Length:' in label_text or 'length:' in label_text.lower():
                        pages_text = value_elem.get_text(strip=True)
                        pages_match = re.search(r'(\d+)', pages_text)
                        if pages_match:
                            detail['pages'] = int(pages_match.group(1))
                            break
        except Exception as e: logging.error(f"è§£æç”»å»Šè¯¦æƒ…æ—¶å‡ºé”™: {e}")
        
        # å¦‚æœæœ€ç»ˆå­—å…¸ä¸ºç©ºï¼Œè®°å½•æ—¥å¿—
        if not detail:
            logging.warning("ç”»å»Šè¯¦æƒ…è§£æç»“æœä¸ºç©ºã€‚é¡µé¢åŸå§‹å†…å®¹å¦‚ä¸‹ï¼š")
            logging.debug(html)

        return detail

    @staticmethod
    def parse_preview_images(html: str) -> List[Dict]:
        previews = []
        soup = BeautifulSoup(html, 'html.parser')
        container = soup.find('div', id='gdt')
        if not container:
            logging.warning("æœªèƒ½è§£æåˆ°é¢„è§ˆå›¾åˆ—è¡¨ (æ‰¾ä¸åˆ°å®¹å™¨ #gdt)ã€‚é¡µé¢åŸå§‹å†…å®¹å¦‚ä¸‹ï¼š")
            logging.debug(html)
            return previews
        
        image_links = container.find_all('a')
        for index, a_tag in enumerate(image_links):
            try:
                div_tag = a_tag.find('div')
                if not div_tag or 'style' not in div_tag.attrs: continue
                style = div_tag['style']
                details_match = EhParser.PATTERN_STYLE_DETAILS.search(style)
                if details_match:
                    width = int(details_match.group(1)); height = int(details_match.group(2))
                    thumbnail_url = details_match.group(3); x_offset = abs(int(details_match.group(4))); y_offset = abs(int(details_match.group(5)))
                    previews.append({'index': index, 'page_url': a_tag['href'], 'thumbnail_url': thumbnail_url, 'crop_x': x_offset, 'crop_y': y_offset, 'crop_w': width, 'crop_h': height})
            except Exception as e: logging.error(f"è§£æå•ä¸ªé¢„è§ˆå›¾æ—¶å‡ºé”™: {e}"); continue
        
        if not previews and image_links:
            logging.warning("é¢„è§ˆå›¾åˆ—è¡¨è§£æç»“æœä¸ºç©ºï¼Œä½†æ‰¾åˆ°äº† a æ ‡ç­¾ã€‚é¡µé¢åŸå§‹å†…å®¹å¦‚ä¸‹ï¼š")
            logging.debug(html)

        return previews

    @staticmethod
    def parse_image_page(html: str) -> Optional[str]:
        soup = BeautifulSoup(html, 'html.parser')
        img_container = soup.find('div', id='i3')
        if not img_container:
            logging.warning("æœªèƒ½è§£æåˆ°å¤§å›¾é¡µé¢ (æ‰¾ä¸åˆ°å®¹å™¨ #i3)ã€‚é¡µé¢åŸå§‹å†…å®¹å¦‚ä¸‹ï¼š")
            logging.debug(html)
            return None
        img_tag = img_container.find('img')
        if not img_tag or 'src' not in img_tag.attrs:
            logging.warning("æœªèƒ½è§£æåˆ°å¤§å›¾ URL (åœ¨ #i3 ä¸­æ‰¾ä¸åˆ°å¸¦ src çš„ img æ ‡ç­¾)ã€‚é¡µé¢åŸå§‹å†…å®¹å¦‚ä¸‹ï¼š")
            logging.debug(html)
            return None
        return img_tag['src']


# ==============================================================================
# æ¨¡å— 2: E-Hentai URL æ„å»ºå™¨ (EhUrlBuilder)
# ==============================================================================
class EhUrlBuilder:
    SITE_E = 'e-hentai.org'; SITE_EX = 'exhentai.org'
    def __init__(self, use_exhentai: bool = False): self.domain = self.SITE_EX if use_exhentai else self.SITE_E; self.base_url = f'https://{self.domain}'
    def build_home_url(self, next_id: Optional[str] = None ) -> str: return f'{self.base_url}/' if not next_id else f'{self.base_url}/?next={next_id}'
    def build_search_url(self, keyword: Optional[str] = None, next_id: Optional[str] = None, **kwargs) -> str:
        params = {}
        if not next_id:
            if keyword: params['f_search'] = keyword.strip()
        else:
            params = {'f_search': keyword.strip(), 'next': next_id} if keyword else {'next': next_id}
        query_string = urlencode(params)
        return f'{self.base_url}/?{query_string}'
    def build_tag_url(self, tag: str, page: int = 0) -> str: encoded_tag = quote(tag); return f'{self.base_url}/tag/{encoded_tag}' if page == 0 else f'{self.base_url}/tag/{encoded_tag}/{page}'
    def build_gallery_url(self, gid: int, token: str) -> str: return f'{self.base_url}/g/{gid}/{token}/'
    def build_popular_url(self) -> str: return f'{self.base_url}/popular'
    def get_referer(self) -> str: return self.base_url

# ==============================================================================
# æ¨¡å— 3: å›¾ç‰‡å¤„ç†å™¨ (ImageProcessor)
# ==============================================================================
class ImageProcessor:
    @staticmethod
    def process_and_compress(image_bytes: bytes, max_width: int, quality: int, crop_params: Optional[Dict] = None) -> Optional[Tuple[bytes, dict]]:
        try:
            img = Image.open(io.BytesIO(image_bytes))
            if crop_params:
                x, y, w, h = crop_params['x'], crop_params['y'], crop_params['w'], crop_params['h']
                box = (x, y, x + w, y + h)
                img = img.crop(box)
            original_width, original_height = img.size
            if original_width > max_width:
                ratio = max_width / original_width; new_height = int(original_height * ratio)
                img = img.resize((max_width, new_height), Image.Resampling.LANCZOS)
            if img.mode in ("RGBA", "P"):
                background = Image.new("RGB", img.size, (255, 255, 255))
                if img.mode == "RGBA": background.paste(img, mask=img.split()[-1])
                else: background.paste(img)
                img = background
            elif img.mode != "RGB": img = img.convert("RGB")
            output = io.BytesIO()
            optimize_options = {"quality": quality, "optimize": True, "progressive": True}
            img.save(output, "JPEG", **optimize_options)
            compressed_bytes = output.getvalue()
            info = {"original_size": f"{original_width}x{original_height}", "compressed_size": f"{img.width}x{img.height}", "file_size": len(compressed_bytes)}
            return compressed_bytes, info
        except Exception as e: logging.error(f"å›¾ç‰‡å¤„ç†å¤±è´¥: {e}"); return None

# ==============================================================================
# æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ (ç‹¬ç«‹çš„ã€å¯ç¼“å­˜çš„å‡½æ•°)
# ==============================================================================
@cached(cache=list_cache)
def get_gallery_list_data(url: str, headers: tuple):
    logging.info(f"ç¼“å­˜æœªå‘½ä¸­æˆ–å·²è¿‡æœŸï¼Œæ­£åœ¨æŠ“å–åˆ—è¡¨é¡µ: {url}")
    html = fetch_page_for_request(url, dict(headers))
    if not html:
        # fetch_page_for_request å†…éƒ¨å·²ç»è®°å½•äº†é”™è¯¯ï¼Œè¿™é‡Œæ— éœ€é‡å¤è®°å½•
        return None
    
    parsed_data = EhParser.parse_gallery_list(html)
    # EhParser å†…éƒ¨å·²ç»å¢åŠ äº†æ—¥å¿—ï¼Œè¿™é‡Œè¿”å›å³å¯
    return parsed_data

@cached(cache=gallery_cache)
def get_gallery_detail_data(gid: int, token: str, headers: tuple, url_builder: 'EhUrlBuilder'):
    url = url_builder.build_gallery_url(gid=gid, token=token)
    logging.info(f"ç¼“å­˜æœªå‘½ä¸­æˆ–å·²è¿‡æœŸï¼Œæ­£åœ¨æŠ“å–è¯¦æƒ…é¡µ: {url}")
    html = fetch_page_for_request(url, dict(headers))
    if not html:
        return None
    
    parsed_data = EhParser.parse_gallery_detail(html)
    # å¦‚æœè§£æç»“æœä¸ºç©ºå­—å…¸ï¼Œè¯´æ˜è§£æå¤±è´¥
    if not parsed_data:
        logging.warning(f"ç”»å»Šè¯¦æƒ…é¡µ {url} è§£æç»“æœä¸ºç©ºã€‚")
        # EhParser å†…éƒ¨å·²è®°å½• HTMLï¼Œè¿™é‡Œåªè®°å½•ä¸Šä¸‹æ–‡
        return None
        
    return parsed_data

@cached(cache=gallery_cache)
def get_gallery_images_data(gid: int, token: str, page: int, headers: tuple, url_builder: 'EhUrlBuilder'):
    url = f"{url_builder.build_gallery_url(gid=gid, token=token)}?p={page}"
    logging.info(f"ç¼“å­˜æœªå‘½ä¸­æˆ–å·²è¿‡æœŸï¼Œæ­£åœ¨æŠ“å–å›¾ç‰‡åˆ—è¡¨å¹¶å¹¶å‘è§£ææ‰€æœ‰å¤§å›¾: {url}")
    preview_html = fetch_page_for_request(url, dict(headers))
    if not preview_html:
        return None
        
    preview_list = EhParser.parse_preview_images(preview_html)
    if not preview_list:
        logging.warning(f"ç”»å»Šå›¾ç‰‡é¢„è§ˆé¡µ {url} è§£æç»“æœä¸ºç©ºåˆ—è¡¨ã€‚")
        # EhParser å†…éƒ¨å·²è®°å½• HTMLï¼Œè¿™é‡Œè¿”å›ç©ºåˆ—è¡¨æ˜¯ç¬¦åˆé¢„æœŸçš„
        return []

    final_images = [None] * len(preview_list)
    def fetch_and_parse_image_url(preview_item):
        page_html = fetch_page_for_request(preview_item['page_url'], dict(headers))
        if page_html:
            image_url = EhParser.parse_image_page(page_html)
            if image_url:
                return preview_item['index'], image_url
        logging.warning(f"æ— æ³•ä» {preview_item['page_url']} è·å–æœ€ç»ˆå›¾ç‰‡é“¾æ¥ã€‚")
        return preview_item['index'], None

    with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_REQUESTS) as executor:
        future_to_url = {executor.submit(fetch_and_parse_image_url, item): item for item in preview_list}
        for future in as_completed(future_to_url):
            original_item = future_to_url[future]
            try:
                index, image_url = future.result()
                if image_url:
                    crop_info = f"&crop_x={original_item['crop_x']}&crop_y={original_item['crop_y']}&crop_w={original_item['crop_w']}&crop_h={original_item['crop_h']}"
                    thumbnail_proxy_url = f"/image/proxy?url={original_item['thumbnail_url']}{crop_info}&w={THUMBNAIL_PROXY_WIDTH}&q={THUMBNAIL_PROXY_QUALITY}"
                    final_images[index] = {'index': index, 'thumbnail_jpg': thumbnail_proxy_url, 'image_jpg': f"/image/proxy?url={image_url}"}
            except Exception as exc: logging.error(f"å¹¶å‘ä»»åŠ¡ç”Ÿæˆå¼‚å¸¸: {exc}")
    return [img for img in final_images if img is not None]


@cached(cache=image_proxy_cache)
def get_processed_image_data(url: str, headers: tuple, max_width: int, quality: int, crop_params: Optional[tuple] = None):
    logging.info(f"å›¾ç‰‡ç¼“å­˜æœªå‘½ä¸­æˆ–å·²è¿‡æœŸï¼Œæ­£åœ¨å¤„ç†å›¾ç‰‡: {url}")
    try:
        response = requests.get(url, headers=dict(headers), timeout=REQUEST_TIMEOUT)
        response.raise_for_status()
        crop_dict = None
        if crop_params: crop_dict = {'x': crop_params[0], 'y': crop_params[1], 'w': crop_params[2], 'h': crop_params[3]}
        result = ImageProcessor.process_and_compress(image_bytes=response.content, max_width=max_width, quality=quality, crop_params=crop_dict)
        return result
    except requests.RequestException as e: logging.error(f"ä¸‹è½½åŸå§‹å›¾ç‰‡å¤±è´¥: {e}"); return None
    except Exception as e: logging.error(f"å¤„ç†å›¾ç‰‡æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}"); return None

# ==============================================================================
# Flask åº”ç”¨é…ç½®ä¸è·¯ç”±
# ==============================================================================
app = Flask(__name__)
app.json.ensure_ascii = False
app.debug = False
REQUEST_TIMEOUT = 20; DEFAULT_PROXY_WIDTH = 400; DEFAULT_PROXY_QUALITY = 50
THUMBNAIL_PROXY_WIDTH = 150; THUMBNAIL_PROXY_QUALITY = 40
MAX_CONCURRENT_REQUESTS = 10

def parse_user_agent(user_agent: str) -> dict:
    device_info = {'product': '', 'brand': '', 'os_type': '', 'os_version': ''}
    try:
        if not user_agent:
            return device_info
        
        parts = user_agent.split('/')
        if len(parts) >= 5:
            device_info['product'] = parts[1] if len(parts) > 1 else ''
            device_info['brand'] = parts[2] if len(parts) > 2 else ''
            device_info['os_type'] = parts[3] if len(parts) > 3 else ''
            device_info['os_version'] = parts[4] if len(parts) > 4 else ''
    except Exception as e:
        logging.warning(f"è§£æ User-Agent å¤±è´¥: {e}")
    
    return device_info

def parse_id(id_str: str) -> tuple:
    try:
        if '_' in id_str:
            parts = id_str.split('_')
            if len(parts) == 2:
                return int(parts[0]), parts[1]
        return None, None
    except Exception as e:
        logging.warning(f"è§£æ ID å¤±è´¥: {e}")
        return None, None

def get_image_params_for_device(device_info: dict) -> tuple:
    default_width = DEFAULT_PROXY_WIDTH
    default_quality = DEFAULT_PROXY_QUALITY
    
    product = device_info.get('product', '').lower()
    brand = device_info.get('brand', '').lower()
    
    if 'band' in product or 'watch' in product or brand in ['xiaomi', 'huawei', 'oppo', 'vivo']:
        default_width = 300
        default_quality = 40
    elif 'phone' in product or 'mobile' in product:
        default_width = 400
        default_quality = 50
    
    return default_width, default_quality

def get_request_context() -> tuple:
    client_cookie = request.headers.get('Cookie')
    use_exhentai = bool(client_cookie and 'igneous=EX' in client_cookie)
    url_builder = EhUrlBuilder(use_exhentai=use_exhentai)
    
    user_agent = request.headers.get('User-Agent', '')
    device_info = parse_user_agent(user_agent)
    default_width, default_quality = get_image_params_for_device(device_info)
    
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7', 'Accept-Language': 'en-US,en;q=0.9', 'Referer': url_builder.get_referer()}
    if client_cookie: headers['Cookie'] = client_cookie
    return headers, url_builder, default_width, default_quality

def fetch_page_for_request(url: str, headers: dict) -> Optional[str]:
    try:
        response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
        response.raise_for_status()
        return response.text
    except requests.RequestException as e:
        logging.error(f"è¯·æ±‚ E-Hentai å¤±è´¥: {e}, URL: {url}")
        return None

@app.route('/test')
def test_page():
    html_content = """
    <!DOCTYPE html>
    <html lang="zh-CN">
    <head>
        <meta charset="UTF-8">
        <title>E-Hentai API æµ‹è¯•å™¨</title>
        <style>
            body{font-family:sans-serif;line-height:1.6;margin:20px;max-width:1200px;margin:0 auto;padding:20px}
            h1,h2,h3{border-bottom:1px solid #ccc;padding-bottom:5px}
            form{margin-bottom:20px;padding:15px;border:1px solid #eee;border-radius:5px;background:#f9f9f9}
            input,button,textarea{padding:8px;margin-right:10px;border:1px solid #ddd;border-radius:4px}
            button{background:#007bff;color:white;border:none;cursor:pointer}
            button:hover{background:#0056b3}
            button:disabled{background:#ccc;cursor:not-allowed}
            pre{background-color:#f4f4f4;padding:15px;border-radius:5px;white-space:pre-wrap;word-wrap:break-word;max-height:600px;overflow-y:auto}
            .container{max-width:1200px;margin:auto}
            .form-group{margin-bottom:10px}
            label{display:inline-block;margin-right:10px}
            .pagination{margin-top:10px;display:flex;gap:10px;align-items:center}
            .page-info{padding:5px 10px;background:#e9ecef;border-radius:4px}
            .config-output{background:#e7f3ff;padding:10px;border-radius:5px;margin-bottom:20px;font-family:monospace}
        </style>
    </head>
    <body>
        <div class="container">
            <h1>ğŸ§ª E-Hentai API æµ‹è¯•å™¨</h1>
            
            <form id="cookieForm">
                <h3>1. è®¾ç½® Cookie (å¯é€‰)</h3>
                <textarea id="cookieInput" rows="2" style="width:100%" placeholder="ä¾‹å¦‚ï¼šigneous=xxx; ipb_member_id=12345; ..."></textarea>
            </form>
            
            <form id="configForm">
                <h3>2. è·å–æ¼«ç”»æºé…ç½®</h3>
                <button type="submit">è·å–é…ç½®</button>
            </form>
            
            <form id="searchForm">
                <h3>3. æœç´¢æ¼«ç”»</h3>
                <div class="form-group">
                    <label>å…³é”®è¯: <input type="text" id="searchKeyword" value="language:chinese" style="width:300px"></label>
                    <label>é¡µæ•°: <input type="number" id="searchPage" value="1" min="1" style="width:80px"></label>
                    <button type="submit">æœç´¢</button>
                </div>
            </form>
            
            <div class="pagination" id="searchPagination" style="display:none;">
                <button id="prevPageBtn">ä¸Šä¸€é¡µ</button>
                <span class="page-info" id="pageInfo">ç¬¬ 1 é¡µ</span>
                <button id="nextPageBtn">ä¸‹ä¸€é¡µ</button>
            </div>
            
            <hr>
            
            <form id="detailForm">
                <h3>4. è·å–æ¼«ç”»è¯¦æƒ…å’Œå›¾ç‰‡</h3>
                <div class="form-group">
                    <label>ID (gid_token): <input type="text" id="comicId" size="40" placeholder="ä¾‹å¦‚: 3645215_4db836130d"></label>
                </div>
                <div class="form-group">
                    <button type="button" id="getDetailBtn">è·å–è¯¦æƒ…</button>
                    <button type="button" id="getImagesBtn">è·å–å›¾ç‰‡</button>
                    <label>ç« èŠ‚: <input type="number" id="chapterNum" value="0" min="0" style="width:80px"></label>
                </div>
            </form>
            
            <h3>ğŸ“‹ ç»“æœè¾“å‡º:</h3>
            <pre id="output">è¿™é‡Œå°†æ˜¾ç¤º API çš„ JSON å“åº”...</pre>
        </div>
        
        <script>
            const cookieInput = document.getElementById("cookieInput");
            const output = document.getElementById("output");
            const searchPage = document.getElementById("searchPage");
            const searchPagination = document.getElementById("searchPagination");
            const pageInfo = document.getElementById("pageInfo");
            const prevPageBtn = document.getElementById("prevPageBtn");
            const nextPageBtn = document.getElementById("nextPageBtn");
            const comicIdInput = document.getElementById("comicId");
            const chapterNum = document.getElementById("chapterNum");
            
            let currentKeyword = "";
            let currentHasMore = false;
            
            cookieInput.value = localStorage.getItem("ehCookie") || "";
            cookieInput.addEventListener("input", () => {
                localStorage.setItem("ehCookie", cookieInput.value);
            });
            
            async function callApi(url, updatePagination = false) {
                const cookie = cookieInput.value;
                output.textContent = "â³ æ­£åœ¨è¯·æ±‚...";
                
                try {
                    const headers = {};
                    if (cookie) headers["Cookie"] = cookie;
                    
                    const response = await fetch(url, { headers });
                    const data = await response.json();
                    
                    output.textContent = JSON.stringify(data, null, 2);
                    
                    if (updatePagination && data.results) {
                        currentKeyword = document.getElementById("searchKeyword").value;
                        currentHasMore = data.has_more;
                        const page = data.page;
                        
                        searchPage.value = page;
                        pageInfo.textContent = `ç¬¬ ${page} é¡µ`;
                        prevPageBtn.disabled = page <= 1;
                        nextPageBtn.disabled = !currentHasMore;
                        searchPagination.style.display = "flex";
                    }
                    
                    if (data.comic_id) {
                        comicIdInput.value = data.comic_id;
                    }
                    
                    if (data.item_id) {
                        comicIdInput.value = data.item_id;
                    }
                } catch (e) {
                    output.textContent = "âŒ è¯·æ±‚å¤±è´¥: " + e.message;
                }
            }
            
            document.getElementById("configForm").addEventListener("submit", (e) => {
                e.preventDefault();
                callApi("/config");
            });
            
            document.getElementById("searchForm").addEventListener("submit", (e) => {
                e.preventDefault();
                const keyword = document.getElementById("searchKeyword").value;
                const page = searchPage.value;
                callApi(`/search?q=${encodeURIComponent(keyword)}&page=${page}`, true);
            });
            
            prevPageBtn.addEventListener("click", () => {
                const page = parseInt(searchPage.value);
                if (page > 1) {
                    searchPage.value = page - 1;
                    callApi(`/search?q=${encodeURIComponent(currentKeyword)}&page=${page - 1}`, true);
                }
            });
            
            nextPageBtn.addEventListener("click", () => {
                const page = parseInt(searchPage.value);
                if (currentHasMore) {
                    searchPage.value = page + 1;
                    callApi(`/search?q=${encodeURIComponent(currentKeyword)}&page=${page + 1}`, true);
                }
            });
            
            document.getElementById("getDetailBtn").addEventListener("click", () => {
                const id = comicIdInput.value.trim();
                if (!id) {
                    alert("è¯·è¾“å…¥æ¼«ç”» ID (æ ¼å¼: gid_token)");
                    return;
                }
                callApi(`/comic/${id}`);
            });
            
            document.getElementById("getImagesBtn").addEventListener("click", () => {
                const id = comicIdInput.value.trim();
                const chapter = chapterNum.value;
                if (!id) {
                    alert("è¯·è¾“å…¥æ¼«ç”» ID (æ ¼å¼: gid_token)");
                    return;
                }
                callApi(`/photo/${id}/${chapter}`);
            });
        </script>
    </body>
    </html>
    """
    return render_template_string(html_content)

@app.route('/')
def home():
    try:
        headers, url_builder, default_width, default_quality = get_request_context()
        next_id = request.args.get('next')
        url = url_builder.build_home_url(next_id=next_id)
        result = get_gallery_list_data(url, tuple(headers.items()))
        if not result: return jsonify({'error': 'æ— æ³•è·å–é¡µé¢å†…å®¹'}), 500
        for gallery in result.get('galleries', []):
            if 'thumbnail' in gallery and gallery['thumbnail']: gallery['thumbnail_proxy'] = f"/image/proxy?url={gallery['thumbnail']}&w={THUMBNAIL_PROXY_WIDTH}&q={THUMBNAIL_PROXY_QUALITY}"
        return jsonify({'success': True, **result})
    except Exception as e: logging.error(f"è·¯ç”± / å‡ºé”™: {e}"); return jsonify({'error': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'}), 500

@app.route('/search')
def search():
    try:
        headers, url_builder, default_width, default_quality = get_request_context()
        search_keyword = request.args.get('q', '')
        page = int(request.args.get('page', 1) or 1)

        keyword = decode_search_value(search_keyword)
        print(f"åŸå§‹å€¼: {search_keyword}, è§£ç å: {keyword}")
        
        if not keyword: return jsonify({'error': 'ç¼ºå°‘æœç´¢å…³é”®è¯å‚æ•° q'}), 400
        
        cache_key = f"search_{keyword}"
        next_id = None
        
        if page > 1:
            next_id = pagination_cache.get(f"{cache_key}_{page - 1}")
            if not next_id:
                for p in range(1, page):
                    prev_next_id = pagination_cache.get(f"{cache_key}_{p - 1}") if p > 1 else None
                    temp_url = url_builder.build_search_url(keyword=keyword, next_id=prev_next_id)
                    temp_result = get_gallery_list_data(temp_url, tuple(headers.items()))
                    if not temp_result:
                        return jsonify({'error': f'æ— æ³•è·å–ç¬¬ {p} é¡µæ•°æ®'}), 500
                    
                    temp_next_id = temp_result.get('pagination', {}).get('next_id')
                    if temp_next_id:
                        pagination_cache[f"{cache_key}_{p}"] = temp_next_id
                    else:
                        return jsonify({'error': f'ç¬¬ {p} é¡µåæ²¡æœ‰æ›´å¤šæ•°æ®'}), 404
                
                next_id = pagination_cache.get(f"{cache_key}_{page - 1}")
        
        url = url_builder.build_search_url(keyword=keyword, next_id=next_id)
        result = get_gallery_list_data(url, tuple(headers.items()))
        if not result: return jsonify({'error': 'æ— æ³•è·å–æœç´¢ç»“æœ'}), 500
        
        current_next_id = result.get('pagination', {}).get('next_id')
        if current_next_id:
            pagination_cache[f"{cache_key}_{page}"] = current_next_id
        
        results = []
        api_url = request.host_url.rstrip("/")
        for gallery in result.get('galleries', []):
            if 'thumbnail' in gallery and gallery['thumbnail']: 
                gallery['thumbnail_proxy'] = f"{api_url}/image/proxy?url={gallery['thumbnail']}&w={THUMBNAIL_PROXY_WIDTH}&q={THUMBNAIL_PROXY_QUALITY}"
            
            results.append({
                'comic_id': f"{gallery.get('gid')}_{gallery.get('token')}",
                'title': gallery.get('title'),
                'cover_url': gallery.get('thumbnail_proxy') or gallery.get('thumbnail'),
                'pages': gallery.get('pages')
            })
        
        has_more = result.get('pagination', {}).get('has_next', False)
        
        return jsonify({
            'page': page,
            'has_more': has_more,
            'results': results
        })
    except Exception as e: logging.error(f"è·¯ç”± /search å‡ºé”™: {e}"); return jsonify({'error': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'}), 500

@app.route('/comic/<id>')
def gallery_detail(id: str):
    try:
        gid, token = parse_id(id)
        if not gid or not token: return jsonify({'error': 'æ— æ•ˆçš„ ID æ ¼å¼ï¼Œåº”ä¸º gid_token'}), 400

        api_url = request.host_url.rstrip("/")
        
        headers, url_builder, default_width, default_quality = get_request_context()
        detail = get_gallery_detail_data(gid, token, tuple(headers.items()), url_builder)
        if not detail: return jsonify({'error': 'æ— æ³•è·å–ç”»å»Šè¯¦æƒ…'}), 500
        detail.update({'gid': gid, 'token': token})
        if 'thumbnail' in detail and detail['thumbnail']: detail['thumbnail_proxy'] = f"{api_url}/image/proxy?url={detail['thumbnail']}&w={THUMBNAIL_PROXY_WIDTH}&q={THUMBNAIL_PROXY_QUALITY}"

        print(detail)
        
        result = {
            'item_id': f"{detail.get('gid')}_{detail.get('token')}",
            'name': detail.get('title'),
            'page_count': detail.get('pages'),
            'rate': detail.get('rating'),
            'cover': detail.get('thumbnail_proxy') or detail.get('thumbnail'),
            'tags': []
        }
        
        if 'tags' in detail and isinstance(detail['tags'], dict):
            for tag_type, tag_list in detail['tags'].items():
                if isinstance(tag_list, list):
                    result['tags'].extend(tag_list)
        
        return jsonify(result)
    except Exception as e: logging.error(f"è·¯ç”± /comic å‡ºé”™: {e}"); return jsonify({'error': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'}), 500

@app.route('/photo/<id>')
@app.route('/photo/<id>/')
@app.route('/photo/<id>/<chapter>')
@app.route('/photo/<id>/<chapter>/')
def gallery_images(id: str, chapter: str):
    try:
        gid, token = parse_id(id)
        if not gid or not token: return jsonify({'error': 'æ— æ•ˆçš„ ID æ ¼å¼ï¼Œåº”ä¸º gid_token'}), 400
        
        headers, url_builder, default_width, default_quality = get_request_context()
        page = int(chapter) if chapter.isdigit() else 0
        processed_images = get_gallery_images_data(gid, token, page, tuple(headers.items()), url_builder)
        if processed_images is None: return jsonify({'error': 'æ— æ³•è·å–ç”»å»Šå›¾ç‰‡åˆ—è¡¨'}), 500
        
        detail = get_gallery_detail_data(gid, token, tuple(headers.items()), url_builder)
        title = detail.get('title', '') if detail else ''

        api_url = request.host_url.rstrip("/")
        
        images = []
        for img in processed_images:
            image_url = img.get('image_jpg', '')
            if image_url:
                images.append({'url': f"{api_url}{image_url}"})
        
        result = {
            'title': title,
            'images': images
        }
        
        return jsonify(result)
    except Exception as e: logging.error(f"è·¯ç”± /photo å‡ºé”™: {e}"); return jsonify({'error': f'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}'}), 500

@app.route('/image/proxy')
def image_proxy():
    image_url = request.args.get('url')
    if not image_url: return jsonify({'error': 'ç¼ºå°‘å›¾ç‰‡ URL å‚æ•°'}), 400
    try:
        headers, _, _, _ = get_request_context()
        max_width = int(request.args.get('w', request.args.get('width', '400')))
        quality = int(request.args.get('q', request.args.get('quality', '50')))
        quality = max(1, min(100, quality))
        crop_params = None
        if 'crop_x' in request.args:
            try:
                crop_params = (int(request.args['crop_x']), int(request.args['crop_y']), int(request.args['crop_w']), int(request.args['crop_h']))
            except (ValueError, KeyError): return jsonify({'error': 'æ— æ•ˆçš„åˆ‡å‰²å‚æ•°'}), 400
        
        result = get_processed_image_data(image_url, tuple(headers.items()), max_width, quality, crop_params)
        if not result: return jsonify({'error': 'æ— æ³•ä¸‹è½½æˆ–å¤„ç†å›¾ç‰‡'}), 500
        
        compressed_bytes, info = result
        response_headers = {"Content-Disposition": "inline", "Content-Length": str(info['file_size']), "X-Image-Original-Size": info['original_size'], "X-Image-Compressed-Size": info['compressed_size'], "Cache-Control": "public, max-age=86400"}
        return Response(compressed_bytes, mimetype="image/jpeg", headers=response_headers)
    except Exception as e: logging.error(f"è·¯ç”± /image/proxy å‡ºé”™: {e}"); return jsonify({'error': f'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}'}), 500

@app.route('/health')
def health(): return jsonify({'status': 'ok', 'client_cookie_provided': bool(request.headers.get('Cookie'))})
        
@app.get("/config")
@app.get("/config/")
def config():
    api_url = request.host_url.rstrip("/")
    return jsonify(
        {
            "E-Hentai": {
                "name": "E-Hentai",
                "apiUrl": api_url,
                "searchPath": "/search?q=<text>&page=<page>",
                "photoPath": "/photo/<id>/<chapter>",
                "detailPath": "/comic/<id>",
                "type": "ehentai",
            },
        }
    )

@app.errorhandler(404)
def not_found(error): return jsonify({'error': 'æœªæ‰¾åˆ°è¯·æ±‚çš„èµ„æº'}), 404

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8000)